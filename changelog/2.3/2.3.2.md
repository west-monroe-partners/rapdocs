# 2.3.2

**Release Issues**

* **Highlight \[This\] source in graphical lineage view**
  * Highlighted \[This\] source object in graphical relations view
* **Sources Waiting on Prior Failures that Don't Exist**
  * 
* **Sources Stuck in Workflow Queue Waiting on Phantom Inputs**
  * 
* **Kill infinte Autoreprocess loops for loopback keyed source with 0 new/changed records.**
  * Keyed sources with 0 changed/new records will no longer kick off Attribute recalculation processes. Preventing looping.
* **Automatically create validation rule for Spark CSV parsing errors**
  * Validation rule "CSV parsing validation" is automatically created for file/delimited/sparky csv parse sources
* **'Save And Reset CDC' Button on source update doesn't kick off CDC process**
  * 
* **Users cannot change from Full to Key refresh type**
  * 
* **Spark CSV Parse Corrupt Data Column Not Populating**
  * Due to Spark bug, csv parser doesn't support renaming column containing corrupt records. When runnins spark csv parser in permissive mode, include \_corrupt\_record STRING into the schema and observe the data in that column. Spaky parse process logs will also include \# of rows with parsing issues
* **Sorting rule by "Last Updated Time" does not work correctly**
  * 
* **NetSuite JDBC**
  * NetSuite Agent JDBC connectivity added as a plugin
* **Data Viewer Removes Extra Spaces**
  * Dataviewer will no longer add extra spaces into data values.
* **Manual Recalculation Results in No Viable Alternative at Input**
  * New enrichment expression parser! Expect a much better experience
* **Pull from Snowflake View Results in ResultType 1004 Not Supported Error**
  * Snowflake Inputs throwing errors during Ingestion, now can run successfully
* **Add copy to clipboard to query**
  * Process parameters now have “copy to clipboard” function when clicking on parameter value
* **Non-reachable related sources show up in enrichment expression source dropdown**
  * 
* **Update enrichment parser**
  * New enrichment expression parser! Expect a much better experience
* **Trim spaces from array values in parameters**
  * Trailing spaces will be trimmed from array parameter values. i.e. key field of "this, that" will correctly parse as "this", "that" instead of "this", " that"
* **Change Key Column Parameter to Reflect raw file column name, and factor in force case insensitive parameter**
  * 
* **Remove All Columns Button too close to Add Column in Output Mapping**
  * 'Remove All Columns' button on Outputs is now positioned to the right, away from the other buttons. It is also a red color to distinguish it from the others.
* **API turning on and off after deployment to 2.3.1**
  * The issue is caused by cloning source with 2 or more primary relations to other sources. During clone, this created primary relation loop which in turn caused endless recursion loop in API when selecting attributes for enrichment.  Based on the customer feedback, we have disabled current clone source functionality and started to work on new extended version to support cloning of multiple pre-configured source packs.  To address this particular issue, we also added a recursion limit in API as a safeguard.
* **Update Private terraform to deploy bastion and VM for accessing UI**
  * Azure environments can now deploy Databricks with VNet injection within the IDO VNet during a public facing deployment
* **Prevent saving enrichments with aggregate functions on :1 related source attributes**
  * New enrichment expression parser! Expect a much better experience
* **Column resize on Agent logs screen doesn't work**
  * 
* **Scrolling down on UI with mousewheel loads one record at a time and columns resize fails**
  * 
* **Remove Agent Code from source List, replace with Connection Name; fix enrichment list spacing for long names**
  * Rule expression sizing has been adjusted to more easily adapt to longer and shorter expressions.   Sources screen no longer has Agent Code but instead has Connection Name as a column and dropdown.
* **Spark CSV Ingestion enhancements**
  * All Spark csv parsing parameters are now available for Sparky Parse processes. See here for more detail: [https://spark.apache.org/docs/3.0.1/api/scala/org/apache/spark/sql/DataFrameReader.html\#csv\(paths:String\*\):org.apache.spark.sql.DataFrame](https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fspark.apache.org%2Fdocs%2F3.0.1%2Fapi%2Fscala%2Forg%2Fapache%2Fspark%2Fsql%2FDataFrameReader.html%23csv%28paths%3AString*%29%3Aorg.apache.spark.sql.DataFrame&data=04%7C01%7CJcrell%40westmonroe.com%7C1c42a32d25d34d5f1d5b08d8f386b5ba%7Cc2a23be5d2cf4e7084ce18adf87d571f%7C0%7C0%7C637527107410368147%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=zJzjq%2FOpAVobHUslIsfo0c11lxBicAySFmh2CrVzpCM%3D&reserved=0)
* **Manage Azure plugin in jars in AutoUpdate Actor**
  * Azure environments will now be able to utilize Agent Plugins
* **RegEx Error - Getting error when inputting a string with bracketed statement inside**
  * New enrichment expression parser! Expect a much better experience
* **Expression Parsing Can't handle two values within the same aggregation**
  * New enrichment expression parser! Expect a much better experience
* **Adjust output mapping card text size for long mappings**
  * Long column expressions on Outputs will now fade and show triple dots in order to not be harshly cut off in the UI.
* **Create connection from Source screen without losing progress on source config**
  * Users can now create a new connection from the Source Settings tab. This action will open a new tab allowing them to make a connection without sacrificing unsaved changes. Once they've made their new connection, they may refresh the connections, and only the connections, by following a dialog prompt on the Source Settings screen. This prompt does not disrupt their unsaved changes.

