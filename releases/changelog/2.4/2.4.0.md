---
description: 5/25/2021
---

# 2.4.0

{% hint style="danger" %}
### ATTENTION!

### Deployment of 2.4.0 can take an hour or longer due to population of new fields in the Meta Postgres database!

Centralized Schedule Management requires us to migrate data from the individual sources to a new object in the Meta Database.

We also are populating new fields in the inputs and processes tables to help display in the UI the Last Run Process to assist with workflow dependency management and manual debugging.
{% endhint %}

{% hint style="info" %}
**Manual Steps:**

A variable called "readOnlyPassword" will need to be added to Terraform, populated by the password for the new read only user for the Postgres Database.
{% endhint %}

## **Version - 2.4.0**

The latest version of Intellio Data Ops is now available. Listed below are the items included in this release.

View a detailed breakdown of the biggest 2.4.0 features in our Announcements Section located [Here](https://intellio.gitbook.io/dataops/v/master/releases/announcements).

### **Hotfixes**

* **Relation expression is incorrectly displayed when viewed from other\(related\) source**  Fixed issue with incorrect display of relation expression when viewed from \[Related\] source. When relation expression used more that one attribute from the Related source, only the first attribute had been flipped to \[This\] container when viewing the relation from \[Related\] source side.
* **Intermittent API failures across all processes**  Added process retry
* **Sanitize \u0000 character in Data profile results**  \u0000 character in data would break Data Profile process. This character is now stripped from Data Profile results
* **S\_Key Reverses Order of Primary Keys When There are Multiple**  For composite keys, the order in which the key was put together was sometimes not matched with the order that user specified.

### **Bugs**

* **Rule Name Updates Only Reflected On First Reference to Rule in dependent Rule expressions**
* **Custom Sources Can't Kick Off Source Level CDC reset**
* **Changing cluster params mid-processing causes infinite queue**
* **Source Update Reset CDC Cancel button saves source instead of cancelling update**
* **Sparky Parse ignoring force case insensitive, causing issues with columns in all caps**
* **Attempting to delete a group results in a 404 error**
* **Manual Reset Output only processing first output of source**
* **Creating agent fails unless you toggle default flag**
* **Rules: No warning on changing rule w/ dependent rules to Keep Current**
* **Import: Loopbacks not populating Virtual Output on Source**  Loopbacks associated with brand new outputs will now populate on import
* **Enrichment attribute name can conflict with raw attribute name**
* **Uncheck primary flag on relation -&gt; can't check it back**
* **Update to Rule Uniqueness Doesn't Update Dependent Relations' Cardinality**

### **Tasks**

* **Move import/clone to a standalone process**  Large clone operations were bringing down the API. We have moved them into a standalone process in the process page to avoid this. The clone process is still the same, but look for the process on the process page after hitting "import"
* **Show scheduled dropdown on sources settings for custom ingest**  Scheduled dropdown now appears for custom ingest on source settings.
* **Snowflake Output using Spark Connector instead of storage integration**  Snowflake Output no longer requires a storage account integration to be set up in S3/Azure Blob storage. We now use the Spark connector to write from Databricks to Snowflake directly
* **Meta\_monitor: refreshed meta.process dataset in hive/databricks**  This feature creates metadata monitoring dataset in databricks: meta.process. This dataset is refreshed periodically \(default setting is 15 min\). Please make sure to run latest Terraform infrastructure script as part of deployment to enable databricks-postgres traffic.
* **Add Agent name hook in Auth0 during Terraform run**  Auth0 hook required for 2.3.0+ versions is now created via Terraform
* **Create release pipelines in bitbucket to build "2.x.x-release" images + deployment container to support them**  Internal release builds now available - to gain early access to new release features, please contact your DataOps representative
* **Convert datatypes in sparky ingest/custom ingest/sparky parse**  Unsupported datatypes in custom ingestion and sparky parse will now be converted to strings.
* **UNiqueness check in output query**  Output queries will correctly filter on user-declared unique fields. Excluding any non-unique related items.
* **Track most recent process in input table**  Most recently completed and current process are now visible on inputs UI and stored in the input table.
* **Workflow queue popup by input**  Added a popup to Source Inputs that appears when inputs are in the workflow queue, purple clock icon. Popup shows workflow information to user from the inputs page.
* **Make output channel a physical column in process table**  Output Channel will now appear as a process scope.
* **Create read only user on first time database deploy**  All environments will now come with a read only postgres user, created by the Deployment service. Terraform must be on the latest IDO master version to utilize this feature. The new meta monitor refresh process will fail without the read only user in the environment. 
* **Add support for date datatype**  Date type is supported
* **Make outputs that do not point at the same output table run in parallel**  Outputs on the same source will now run in parallel.
* **Add Tooltips for source name in processing tab**  The Source scope will now show the associated scope on hover over.
* **Custom Post-Output SDK + Custom Loopback**  Custom post output is now available. Documentation here:
* **Data Viewer performance on super-wide tables**  Dataviewer now uses limited column pagination to reduce load time.
* **Auto-managed views for each hub table using source name**  Added "hub\_view"name" parameter to the source settings tab, populated automatically from source name and can be updated by user. It generates databaricks view pointing to the hub table for the source. Clicking hyperlink next to view name opens new browser tab in databricks with view details and data preview
* **When saving an output, it should navigate me to the mapping screen**  Saving output will now navigate to mapping screen automatically
* **As a user, I want to manage my schedules centrally \(similar to Connections\)**  Schedules has moved from Source Settings to its own page. Sources now are linked to a schedule from the source settings page.   Schedules enables users to make and name schedules, with parsed CRON.
* **Add spark parameter to enable databricks ver. 7.3+**  IDO now supports Databricks 7.3
* **DOC - Configuration Guide - Sources - Relations**  Relations section of documentation is complete and up to date.
* **Upgrade SBT to 1.4 and Scala 2.12**  Updated builds to use SBT 1.4 and run on minimum Scala 2.12.13

Please contact your support representative with any questions or feedback.

