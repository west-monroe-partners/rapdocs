---
description: 06/03/2021
---

# 2.4.0 - June 3 Hotfix

We strongly recommend that all environments be updated to this release as soon as possible. It contains many enhancements to increase job and API stability. Rare cases of data duplication can occur if this hotfix is not applied.

## **Job Stability**

Prior to this hotfix, under times of high load, DataOps could occasionally launch two clusters to perform the work of a single process. In some cases, this led to data duplication within hub tables. With data integrity being the top priority of all DataOps processing, we have implemented multiple redundant measures to ensure this cannot happen in the future. In the event that _**any**_ abnormal patterns are detected, DataOps will immediately cancel the job in question and recalculate the jobs needed. With this is place, some failed and retried processes will be more common.&#x20;

The redundant safety checks are as follows:

* More than one job record executing the same process id
* Any discrepancy between the job ids in the process table and the job ids listed in the spark job table
* A spark job with no corresponding spark\_job record making a call to the API
* A spark job with a process\_id not present in the process table making a call to the API

## Custom Cluster Stability

Prior to this hotfix, failures were frequent for clusters that were not standard DataOps Managed clusters. This was due to a hardcoded 5 minute maximum wait time for jobs to start after being created. With occasionally very complex clusters, 5 minutes was not long enough and processes would fail as a result. In this hotfix, all sources now have a Max Heartbeat Wait Time parameter to make this configurable.  We recommend updating this parameter anytime a source fails with errors related to cluster launch heartbeats.

## Resource Management Stability

In rare cases, jobs would get stuck as "waiting for cluster" in the process queue with no obvious cause. We have added a parameter, max job launch wait time, to every source that will allow users to control how long a process should wait in "waiting for cluster" status before going ahead and launching a new job.

## API Stability

Beyond interrupting usage of the UI, API failures can also cause processing issues within data processing jobs. In order to reduce API interruptions, the process page has been updated to no longer auto refresh. This query was found to be very resource intensive and could crash the API when multiple tabs were open at once. We hope to restore this functionality soon.

## Sparky Parse w/ Case Sensitive Sources

Sources that passed through sparky parse were incorrectly recording their metadata when the source was marked case sensitive and mixed-casing column headers existed. This caused keyed sources to write null values into those columns. This is now fixed.



