---
description: This release adds new features as well as fixes bugs found in 2.5.1
---

# 5.1.0

{% hint style="info" %}
Please see the blog post on [Versioning Update Post 2.5.0](../../announcements/versioning-update-post-2.5.0.md) for information on the new standards for version numbers moving forward.
{% endhint %}

### Bugs

* **Add ability to clear non-required drop downs in source settings**\
  Non-required search dropdowns can now be cleared. Schedules is no longer a required parameter in Source settings.
* **Enrichment attribute dropdown does not show window/keep current enrichments from \[This] source**\
  In 2.5.0, window function rules and rules that depended on them disappeared from the autocomplete dropdown that displays when making an enrichment, relation, or output column mapping. These rules have been added back.
* **Can't add timestamp to a output source.**\
  Adding \<ts> to append the current timestamp on the end of an Output File Mask was causing failues. This has been fixed.
* **Output: Saved success message is no longer available**\
  Output mapping now shows a snackbar message when automap is used on existing mappings
* **Azure Test cleanup is throwing error message**\
  N/A
* **Cleanup: Inputs No Effective Range Period parameter should not delete files when Refresh type is full**\
  N/A. Bug found in QA of new feature and fixed.
* **Clicking on Cleanup Configuration dropdown to select a different configuration is giving 400 Error**\
  Cleanup configuration dropdown no longer gives a 400 error
* **Add a option to Delete a cleanup configuration like cluster configuration**\
  Cleanup Configurations can be deleted
* **Remove cleanup statement from log message when cleanup is not done for a particular Source**\
  N/A. Applies to another new feature in this release.
* **Default Cleanup Configuration value is not saving**\
  N/A. Bug found in QA of new feature.
* **Agent eats error when database query fails for any reason**\
  Fixed the issue with malformed source query error not being captured in logs\
  Removed initial record count from table sources, speeding up ingestion.
* **Blank source status when processes/inputs have run and have data**\
  Updated the logic for Source Statuses to avoid an ege case that wrote NULL values.
* **Failed Output still marks input record as P status if Data Profile passes**\
  Data Profile no longer will mark failed inputs as P if it completes after the failures.
* **UI-Clicking on a sub-menu items resets tab index to zero**\
  Clicking on sub-menu items no longer forces users from their tabs
* **Source: Schedule type set to watcher trigger double ingest**\
  Watcher initiation type will no longer have the source file type option for file ingestion and multipart file ingestion using Watcher type is deprecated.
* **Remove Cost column from Process page**\
  Processing Queue no longer has the cost column.
* **Connection ID filter on Sources shows more than one connection**\
  Source filtering on connection ids is now fixed
* **Hard dependencies not brought through in cloning and/or import/export**\
  Dependencies will now be cloned properly. Dependencies between sources in the same group will create new dependencies between the new versions of those sources (i.e. Source ${GROUP} A depending on Source {GROUP} B)\
  While grouped sources depending on non-grouped sources will create new dependencies between the new grouped source and the already existing non-grouped source (i.e. Source ${GROUP} depending on Master Data - Products)
* **Data Viewer "Download" only gives you columns that show on screen, instead of columns that you have selected**\
  Data Viewer download button now downloads data from all columns that are selected in the selection modal.
* **Multipart file uploads need to show combined file size on inputs**\
  Inputs that are ingested using "multipart" for the source file type parameter now properly display their size on the Inputs page.
* **Searching by Input ID in Processing Queue Does Not Result in Correct Source**\
  Input id filter has been removed from global processing page
* **Ingestion and Sparky Parse don't show up in "current process type" on inputs page**\
  Ingestion and Sparky Parse processes now are displayed in the "Current Process Type" column on the Inputs page when appropriate.

### Tasks

* **Add job\_run\_id to log.actor\_log table**\
  The log.actor\_log table, which tracks all IDO processing logs, now has a job\_run\_id column to allow better tracability between individual Databricks jobs and the processing work that they did.
* **Remove SFTP as an Output Connection type**\
  Removal of deprecated parameters: SFTP output, CSV parameters on File outputs
* **Throw an error on enqueue if output\_source\_column is mapped to a field that does not exist**\
  If an output channel is mapped to an enrichment/raw attribute that does not exist (an edge case that should not happen but has been seen when doing import/export with signifigant column name changes) IDO will now throw an error and alert the user to the missing column for remapping.
* **Hide automap channel parameters**\
  Removed non-functional parameters from output channel settings:\
  Auto\_Add\_Columns\
  auto\_column\_name\_filter
* **Use source\_schema table to get enrichment renames rather than process history**\
  Changed logic for updating column names when enrichment attribute\_names change to prevent IDO from trying to rename columns twice.
* **Logs written close together appear in UI out of order**\
  Process logs will now do a better job of appearing in order.
* **Add Applied Objects tab to top level UI pages**\
  A new tab has been addded to the pages for individual Connections, Schedules, Groups, Cluster Configurations, Process Configurations, Source Name Templates, Output Name Templates, and Connection Name Templates. It will show all of the IDO objects that use the object. i.e. When looking at Schedule A the Applied Objects tab will show that it is used on Source A, B, and C.
* **ECS agents (usage in particular) should ignore Auto Update parameter**\
  ECS/Azure container agents will no longer check the Auto Update parameter, as they are not able to auto update. These agents are updated through docker image changes via Terraform version upgrade process
* **Enrichments still failing to save with special characters in relation names**\
  All special characters can now be used in source names, enrichments, and relations except square brackets \[] and curly brackets {} which are reserved for IQL parsing.
* **Change default view to tabular on relations page**\
  Navigating to the "Relations" tab on a source will now show the Table view rather than the Graph view by default
* **Add waits to other processes when cleanup is running to avoid data loss**\
  Updated wait/workflow logic so that cleanup will not run at the same time as source processing.
* **Delete process configuration**\
  The Process Configuration Settings screen now has a "Delete" button.
* **Add error handler + retry to Agent API calls**\
  Agent API calls will retry on errors with a backoff strategy, rather than crashing and restarting the Agent immediately.
* **"Delete File" Parameter does nothing on Watcher sources**\
  Updated parameter description
* **Prevent saving enrichment attribute\_name that matches the old name of another enrichment of the same source**\
  When renaming enrichment attribute\_names, IDO will now throw an error if the user attempts to rename a column in a way that would break IDO's ability to update the hub table schema.
* **Update Gitbook to reflect recent changes**\
  Update complete.
* **Create Hotfix Dev environment in AWS and Azure, update pipelines to auto deploy to it**\
  Hotfix development pipelines created internally
* **Job run UI page build**\
  New Feature - Job Runs table added to UI\
  Users can now see all job run details from state, to stop reason, to launch date and more from the UI
* **Cleanup db log (archive)**\
  The IDO Cleanup process will archive processing logs as files in s3/Azure Blob and remove them from the backend database.
* **Cleanup delete executor build**\
  Cleanup has been upgraded. IDO now cleans up the data and files for deleted sources, cleans up old versions of hub tables, and archives logs and processes in S3/Azure Blob.
* **Add "Force Reorchestrate" button to the top level workflow tab**\
  New "Recheck Queue" button added to workflow queue tab that re-orchestrates queue when stuck
* **Optimize workflowRelease function in core to enable scaling for large # of concurrent jobs**\
  Moved recursive workflow release logic from core to sparky\
  Removed redundant sparky-core API calls\
  Removed redundant DB calls from core (getLogId)\
  Implemented small queue-like table to process releases to ensure sequential release processing and improve resilience
* **Update UI link to docs to new domain**\
  Documentation link on menu has been updated to the new domain.
* **Agents flooding AWS acces event logs with "AccessDenied" errors while executing s3 GetBucketAcl API call**\
  Agent logs will no longer be flooded with errors if Agent is unable to access a file location for watcher initiation types. After three errors, the source will have initiation disabled on it.
* **As a user, I want to clone a source with an output and have the option to have it create a new output rather than add a channel to existing output**\
  Output Name Templates and Connection Name Templates have been added to make the cloning experience easier. Check out the Cloning section in Gitbook for more details.
* **Create centralized data retention object for Deletes**\
  Cleanup settings are now a top level object similar to schedules and connections. Users will be able to make a "Cleanup Configuration" that specifies source retention settings and apply the configuration across multiple sources.
* **bundle up input deletes for non-keyed refresh sources**\
  Input deletes are bundled up into one process if multiple inputs are selected
* **Cleanup orphaned datalake files**\
  After deleting a source, IDO will now delete all of the deleted source's files from S3/Azure Blob in the next cleanup run.
* **Update S3 datalake to intelligent tier policy by default**\
  S3 datalake has Intelligent Tiering lifecycle policy added. After 1 day in the datalake, Intelligent Tiering will be applied to objects larger than 256kb.
* **Make Delete Template Functionality**\
  Rule Templates and Relation Templates can be deleted if there are no linked sources.

Please contact your support representative with any questions or feedback.

