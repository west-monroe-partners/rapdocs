---
description: 01/20/2022
---

# 2.5.0

The latest version of Intellio Data Ops is now available. Listed below are the items included in this release:

Hotfixes

* **Converting Output from Virtual to Delta Lake breaks Output partitions**\
  Outputs can be converted from Virtual to Delta Lake with no errors during next processing on Delta Lake configuration

Bugs

* **Process Config and Cluster Config go off the screen when they're too long - also causes the click to edit and new buttons to disappear**\
  Long dropdown names now are shortened to triple dots.\

* **Source Settings Connection is not marked as required**\
  Connections is now marked required for all connection types except custom\

* **Delta lake tables do not drop columns when removed from output definition**\
  Delta lake tables drop column now when removed from output definition\

* **Long names overflow screen width on top navigation dropdown**\
  Long names in top navigation are shortened with an ellipsis.
* **process.start\_datetime is not populated during table ingestion**\
  Table ingestion populates process.start\_datetime when the process is created and started.
* **Workflow modal is broken**\
  Fixed the issue with workflow modal window on individual input
* **Can't save a source after changing CDC parameter**\
  n/a
* **Source Settings indentation gone rogue**\
  Radio button indentation has been updated for all settings pages; Sources has also moved group and source template dropdowns.
* **Metadata dialog throws console error if metadata is blank**\
  Raw Schema tab's metadata icon now is now disabled/greyed out when there isn't any metadata.
* **Error on Recalculate All on Full Source**\
  N/A
* **Uninformative error when new ido-data-viewer is off and try to create rule**\
  IDO Data Viewer will now use a separate cluster than mini sparky
* **Input status shows "in-progress" when the cluster is actually launching for Custom Ingest**\
  Custom ingestion sources will now correctly show the "Launching" status while a cluster is launching and not yet started
* **Custom Ingest doesn't retry**\
  Custom ingestion will now retry based on the max retries parameter in source settings
* **New cluster config creation: Changing toggles wipes out existing parameters**\
  n/a
* **Filtering sources pane by connection pane results in forever spinning page and no results**\
  n/a
* **Connection Filter fails on Sources List**\
  Connections filter on Sources page no longer fails to filter.
* **Array fields are saving as plain strings when popups are thrown on screen**\
  Array and JSON objects now both save properly.
* **When data profiling finishes processing in 2.5.0, it still says "current process Data Profile" on the inputs tab**\
  n/a - 2.5 bugfix
* **Error in Relation Template Validation shows Object Object**\
  Relation Template validation error no longer shows Object Object
* **Relation Template Creation Page Shows Incorrect Cardinality**\
  Relation templates now show correct cardinality.
* **Output Mappings saving broken with several components**\
  All save functions on output mappings are working once more.
* **New output channel doesn't save**\
  n/a
* **Hostname Always Shows "0.0.0.0"**\
  Agent will log hostname and IP of server it is running on on startup
* **Stuck getting errors after failing to save Cluster Config**\
  N/A
* **Improve logging/errors around blank raw attribute names (file with no headers)**\
  Files with blank raw attribute names will now fail in Core parsing rather and not add the blank attribute to the raw attribute table
* **Core doesn't check custom ingestion pull now flags after they were first clicked**\
  Core will check pull now flag on custom ingestion sources more frequently now to make sure custom ingestion gets queued in a timely manner.
* **Viritual Output not required on Loopback sources**\
  Virtual Outputs are now required for Loopback sources,
* **Using relation in data viewer filter expression results in error**\
  "Relations cannot be used on this page" message will now display if a user attempts to press \~ in the filter expression on the dataviewer page
* **MAX\_BY aggregate function not recognized as a valid aggregate function**\
  All spark sql aggregates are now supported by IDO in enrichment expressions. See full list here: [https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html](https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html)
* **Output failure isn't reflected on source level status**\
  Source status update issue fixed
* **AND clause causing relation parameters to be reassigned to wrong source on relation re-save**\
  Cannnot Reproduce

Tasks

* **Raw Schema Documentation**\
  n/a
* **Manual smoke test to make sure rule templates work correctly in 2.5.0**\
  n/a
* **Add Azure-specific attributes to cluster configuration parameters**\
  Added Azure-specific databricks configuration options
* **Test 2.5.0->2.4.3 rollback and document process**\
  2.4.3 rollback is available
* **Immediately fail a Sparky process if it tries to enqueue without a cluster configuration id**\
  \

* **Add access denied exception to Ingestion logs rather than "file does not exist"**\
  Ingestion logs will show clearer errors during scheduled file ingestion
* **Remove hub table delete from reset ALL CDC API call**\
  N/A
* **Cluster Configuration JSON parameter merging**\
  Additional spark\_conf or libraries added in Custom Cluster configuration will be merged with our default IDO conf and libraries behind the scenes
* **Provide user feedback when reset recalculate does nothing**\
  Recalculate button will now display a snackbar message to the user that says if it enqueued a recalculate process or not
* **Import/Export works with new cluster configs**\
  Process configurations will be exported between environments based on their names.
* **Private UI Terraform updates**\
  N/A
* **Update Bitbucket pipeline and Deployment container to utilize private UI container image**\
  Private UI container is built in bitbucket now
* **Create Dockerfile for private UI container**\
  N/A
* **Retry process for input\_delete does not copy process.parameters.inout\_ids from the failed process**\
  N/A unreleased bug during development of 2.5.0
* **Include a "restart agent" button so client doesn't have to do a manual table update for meta.agent**\
  Agents table now has a triple dot menu with a restart agent button. This button allows users to restart the agent through the UI instead of a manual table update.
* **Test 2.4.3 -> 2.5.0 environment migration and deployment**\
  2.4.3 -> 2.5.0 database migration has been ran through
* **Add cluster launching status to "In Progress" filter on inputs tab**\
  Cluster launching will now show when the In Progress filter is selected on the Sources Inputs tab
* **Remove hub table check process queuing from import**\
  Hub table check processes will no longer be run during imports. They will be handled by enrichment & refresh during actual data processing
* **Create legacy E2 -> E2 migration guide in GitBook**\
  Created documentation for migrating legacy Databricks environment in AWS to new E2 Databricks workspace
* **Change result file type avro->parquet for custom ingestion, custom & sparky parse**\
  This improves performance by 50-80% over the write-read cycle. The result is smaller and better compressed files. The system will now automatically recognize the file type and handle legacy avro-formatted files.
* **Sparky processSchema overwrites hub table with incorrect # of partitions and ignores bucketing options**\
  IDO optimizes size of each individual parquet file (partition) for hub tables using the value of target-parquet-file-size global parameter (default 256MB) stored in meta.system\_configuration table. This optimization was ignored/overwritten during hub table schema update, after source schema change. This fix retains correct, optimal number of hub table partitions during schema changes
* **Migrate process\_history to history.process**\
  Move process\_hisory to history.process and migrate data
* **Add 2.4.3->2.5 migration script for custom ingest, parse and post-output**\
  Automatically migrate pre-2.5 custom job and cluster configurations
* **Migrate Sandbox to E2 environment, destroy and rebuild Test and Demo2 using E2**\
  Developed and tested strategy for migrating non-E2 Databricks environments to E2
* **Add right-click capability to list components**\
  All list pages, such as Sources, Outputs, Schedules, etc. now allow right-click abilities. Uses can either right-click to open a page in either a new window or tab, or ctrl-click to open in a new tab. Left clicking will navigate users in the same tab and window.
* **Reset All buttons give no response if Mini-sparky is off or starting up**\
  When clicking a reset source button, such as "Reset All CDC" or "Delete Source Data", a snackbar message will now tell users if the Mini Sparky cluster is off or starting up. Previously, no message would show and nothing would happen on the page.
* **Replace API calls with direct DB calls in Custom SDK**\
  As part of Sparky communication refactoring, we removed API layer from SDK job to Postgres DB communication. Each SDK job is now communicating directly to the database, in the same manner as standard IDO "sparky" job. This improved reliability, resiliency and scalability.
* **Create databricks secret containing the value of AWS/Azure "/connection-parameter-key" secret as base64-encoded string**\
  N/A
* **Cut out Sparky Auth0 token components**\
  Sparky no longer communicates to the API using Auth0 or Auth0 tokens.
* **Update data-lake-path in meta.system\_configuration**\
  data-lake-path added to meta.system\_configuration
* **Custom SDK Refactor for new Cluster Launch/Orchestration**\
  Custom SDK code has been refactored to support new Cluster & Job configuration.\
  Cluster Configurations for Custom Ingest and Parse are selected on Source Settings tab, and custom post-output configuration is selected on the output settings tab.\
  Each custom cluster configuration now includes clickable link to access notebook details.\
  As of 2.5 release, we've deprecated support for custom jars and only support notebooks.
* **Update Core container security group to allow inbound traffic on 7131 from databricks subnets**\
  N/A
* **Add "etl-url" to meta.system\_configuration**\
  etl-url has been added to meta.system\_configuration
* **Prevent concurrent runs for hub table check**\
  Two known issues have been fixed:\
  \- Under high load, IDO could execute concurrent hub table check operations on the same hub table. This could lead to race condition and risk of table corruption or data duplication\
  \- Hub table check operation ignored target-parquet-file-size global parameter, overwriting the table with number of partitions equal to total number of spark cluster cores. This resulted in sub-optimal table partitioning and data skew
* **Deploy Azure Major Environment- Hook up to major/2.x.x branch**\
  Created major branch environment for major feature testing
* **Add cluster autoscale checkbox to parameters**\
  Centralized cluster configuration now supports automatic scaling of databricks clusters, enabled by default.
* **Estimate file size and optimize partition count by process type**\
  Pre-2.5 release, IDO optimized size of each individual parquet file (partition) for hub tables using the value of target-parquet-file-size global parameter (default 256MB) stored in meta.system\_configuration table.\
  In this release, this is extended to all layers of the Datalake from parsed through hub table files. This will improve processing performance of large batches of data (inputs) across all process types from CDC through Attribute Recalculation by optimizing spark read and write performance, parquet compression and minimizing data skew in spark dataframe partitions.\
  This also enables cost-effective databricks cluster auto-scaling as number of file partitions is determined by size of the batch and source data and is no longer tied to the number of compute cores configured for the cluster.
* **Update sparky databricks secrets**\
  Certain secrets have been moved to Databricks secrets
* **Remove EMR code from API, Core, Terraform**\
  EMR support has been removed
* **Remove EMR code from Sparky**\
  Removed & deprecated EMR spark cluster support
* **Refactor process\_history => history.process**\
  meta.process\_history has been renamed to history.process and all records were migrated
* **Prevent accidental upgrade to 2.5.0**\
  AWS environments that are not utilizing Databricks E2 architecture will be blocked from upgrading to 2.5.0 by the deployment container.
* **Remove API calls in Sparky and call Postgres functions/db directly**\
  Pre-2.5, each spark job communicated with IDO database using an API layer. This often led to instability under high load and caused jobs to be aborted due to transient API load balancer errors, resulting in increased costs for reprocessing.\
  New Databricks E2 architecture simplifies spark job communications, pointing directly to the Postgres database and bypassing the API layer. This change enabled several benefits:\
  \- Improved stability, eliminating API layer as the point of failure\
  \- Reduced reprocessing costs\
  \- Reduced API load, costs, and improved UI performance and user experience under high load\
  \- Enabled dynamic scale of Aurora RDS instance with load, reducing AWS costs
* **Create GitBook documentation for Cluster Configuration**\
  n/a
* **Document how to configure a cluster to avoid Lack of Spot Instance Avail. Error**\
  [https://docs.dataops.intellio.wmp.com/dataops/v/master/user-manual/system-configuration/cluster-and-process-configuration-overview/cluster-configuration/spot-instance-configuration](https://docs.dataops.intellio.wmp.com/dataops/v/master/user-manual/system-configuration/cluster-and-process-configuration-overview/cluster-configuration/spot-instance-configuration)
* **Optimize Reset All Output**\
  Reset All Output button will now run a single process per output rather than a single process per input.
* **Switch custom ingestion step to use custom\_ingestion process type**\
  Custom ingestions will now run a "Custom Ingestion" process rather than "Ingestion"
* **Create Mini sparky using terraform rather than API in E2 environments**\
  Mini Sparky is created using Terraform now. Data Viewer cluster will also be created with Terraform and will be used for Data Viewer access
* **Improve processing time for Keyed CDC/Refresh reprocessing**\
  Pre-2.5 resetting CDC or deleting input in source with key refresh took a long time; this required rollback process and sequentially executed CDC reset operations for each individual input which could take several hours or even days on a source with many inputs.\
  Now, there is a new manual\_reset\_all\_processing\_from\_cdc process type to optimize performance of these operations. As a result, the rollback process type is now deprecated.\
  Both “input delete” and “reset all CDC” operations use the new process type and are now performed as a single spark job. For input delete, the job reads all CDC files, recalculates record\_status\_code, performs enrichment and overwrites hub and hub\_history tables. Reset All CDC starts with parsed files and recalculates key and s\_update\_timestamp CDC columns before following the same process.\
  To have the best processing performance for large sources, users are recommended to have auto-scaling cluster with 4-8 core node size and large max number of nodes, minimum ten, configured for this operation.
* **Output Mapping - Save and refresh after every major action**\
  \

* **Allow clients to use existing VPC instead of DataOps created VPC during Terraform deploy**\
  There are now added variables so users can specify existing VPC and subnets for IDO to deploy into in AWS
* **Centralize custom cluster configuration**\
  Databricks cluster & job configuration has be centralized as a new configuration object accessible under System Configuration top level menu. Please see details in documentation
* **STYLE Inputs Workflow Dialog Updates**\
  Workflow Dialog popup now has better column spacing, wait details wraps to a new line, and wait type is properly parsed.
* **Add support for custom keys for AWS s3 file connections (outputs)**\
  AWS S3 File output connections can now use IAM access keys instead of instance profile attached to cluster. See new cloud credentials parameter in Connections
* **Allow spaces in output column names where possible**\
  Ready for release

Please contact your support representative with any questions or feedback.

\


Thank you for your continued use and support of West Monroe's Intellio DataOps,

Intellio DataOps Team\
\
_**Copyright (c) 2021 West Monroe Partners**_\
_**All rights reserved.**_
