# 2.1.0

## Upgrade Instructions

### **Creating a New Databricks Pool**

1\.       Log into Databricks, navigate to Clusters, then to Pools

2\.       Click Create New Pool

3\.       Configure the pool to match the below image (note that availability zone options may differ depending on where your instance of DataOps is deployed.)

![](<../../../../.gitbook/assets/image (306).png>)

4\.       Click Create. Copy the pool ID, which is found in the url (an example is below.)

![](<../../../../.gitbook/assets/image (307).png>)

### **Updating AWS Secrets/Azure Secrets**

**AWS**

1\.       Navigate to AWS Secret Manager

2\.       Find the \<Env>/public-system-configuration secret and click into it

3\.       Click Retrieve Secret Value

4\.       Click Edit

5\.       Find the databricks-instance-pool-id key. Update the value to match the one copied in step 4.

6\.   Find the databricks-main-sparky-config key.  Update the value 6.6.x-scala2.11 to 7.2.x-scala2.12

7\.   Find the databricks-mini-sparky-config key. Update the value 6.6.x-scala2.11 to 7.2.x-scala2.12

**Azure**

**1.** Navigate to your Azure resource group and open your Key Vault. &#x20;

2\. Navigate to Secrets in the left hand pane

3\.  Find the \<Env>/public-system-configuration secret and click into it

4\. Click into the current version of the secret

5\. Copy the secret value and paste into a text editor

6\.   Find the databricks-instance-pool-id key. Update the value to match the one copied in step 4.

7\.   Find the databricks-main-sparky-config key.  Update the value 6.6.x-scala2.11 to 7.2.x-scala2.12

8\.   Find the databricks-mini-sparky-config key. Update the value 6.6.x-scala2.11 to 7.2.x-scala2.12

9\. Repeat steps 5-8. Click New Version + and paste your new secret value into the Value field. Click Create.

### **Updating Intellio Hub Tables - Part 1**

**AWS**

1\.   Navigate to RDS

2\.   Click DB Instances

3\.   Click into the DB for the environment

4\.   Click Actions -> Query -> Authenticate with the stageuser username and password. These can be found in the private-system-configuration secret if needed

**Azure**

1\.   Connect to your Postgres database.

### **Updating Intellio Hub Tables - Part 2**

1\.   Run the following query: SELECT string\_agg(source\_id::text, ', ') FROM meta.source WHERE refresh\_type = 'full'

2\.   Copy the query result.

3\.   Navigate to Databricks

4\.   Open a Notebook

5\.   Copy the below code into a notebook cell

`val db = "test"`

`val organization = "wmp"`

`val datalake = s3://lake`

`val ids = Vector( 1618, 2612, 2718)`

`ids.foreach(id => {`

&#x20; `if (spark.sqlContext.tableNames(db).contains("hub_" + id)) {`

&#x20; `val df = spark.sql("SELECT * FROM " + db + ".hub_" + id)`

&#x20;   `try{`

&#x20; `val inputId = spark.sql("SELECT max(s_input_id) FROM " + db + ".hub_" + id).collect.head.getInt(0)`

&#x20; `df.coalesce(8).write.partitionBy("s_input_id").format("parquet").option("path", datalake  + "/source_" + id + "/hub/history/0").mode("overwrite").saveAsTable(db + ".hub_history_" + id)`

&#x20; `spark.sql("DROP TABLE " + db + ".hub_" + id)`

&#x20;   `spark.sql("CREATE OR REPLACE VIEW " + db + ".hub_" + id + " AS SELECT * FROM " + db + ".hub_history_" + id + " WHERE s_input_id = " + inputId)`

&#x20; `}}})`

6\.   Replace the numbers in the ids value with the ids from step 16 & 17.

7\.   Replace the db with your databricks db name. Check the Data tab to the left if you are unsure of this name.

8\. Replace the datalake with the below value depending on your cloud type

**AWS -** "s3://" + db + "-datalake-" + organization

**Azure -** dbfs:/mnt/datalake

9\.   Replace the organization with your organization ID as used within DataOps. If you are unsure of this value, check the end of your S3 bucket names in AWS.

10\.   Run the Notebook.



```
    Release notes - DataOps Dev - Version 2.1
```

&#x20;Hotfix

* \[[DEV-2021](https://wmpartners.atlassian.net/browse/DEV-2021)] - Agent Jar isn't signed

&#x20;Task

* \[[DEV-1905](https://wmpartners.atlassian.net/browse/DEV-1905)] - Prevent Users from Updating Sources In Relation
* \[[DEV-1907](https://wmpartners.atlassian.net/browse/DEV-1907)] - Update key refresh for better performance
* \[[DEV-1921](https://wmpartners.atlassian.net/browse/DEV-1921)] - Can't open things in new tab in UI. (Process Page)
* \[[DEV-1932](https://wmpartners.atlassian.net/browse/DEV-1932)] - Columns from deleted inputs/source data remain in raw\_attribute table
* \[[DEV-1933](https://wmpartners.atlassian.net/browse/DEV-1933)] - s\_row\_id - move logic to parse/ingestion
* \[[DEV-1936](https://wmpartners.atlassian.net/browse/DEV-1936)] - Add output-source level parameter to control output refresh mode
* \[[DEV-1943](https://wmpartners.atlassian.net/browse/DEV-1943)] - mask\_parsing parameter on ingestion
* \[[DEV-1966](https://wmpartners.atlassian.net/browse/DEV-1966)] - Handle version numbers correctly with aliased columns in raw\_attribute table
* \[[DEV-1967](https://wmpartners.atlassian.net/browse/DEV-1967)] - Add parameter to lowercase columns (forced case insensitive) when capturing raw metadata
* \[[DEV-1970](https://wmpartners.atlassian.net/browse/DEV-1970)] - Update Tab links to use Names instead of Indices
* \[[DEV-1971](https://wmpartners.atlassian.net/browse/DEV-1971)] - Add Tab URL Navigation to other Pages
* \[[DEV-1972](https://wmpartners.atlassian.net/browse/DEV-1972)] - Build relations lineage view
* \[[DEV-1977](https://wmpartners.atlassian.net/browse/DEV-1977)] - Set Sparky Parse to csv failfast mode instead of permissive mode
* \[[DEV-1979](https://wmpartners.atlassian.net/browse/DEV-1979)] - Keyed source deletes
* \[[DEV-2013](https://wmpartners.atlassian.net/browse/DEV-2013)] - Add ability to chain together multiple x-to-1 relations in output mapping screen
* \[[DEV-2054](https://wmpartners.atlassian.net/browse/DEV-2054)] - Stop 'Agents Logs' component from re-rendering/call parameters

&#x20;Bug

* \[[DEV-1915](https://wmpartners.atlassian.net/browse/DEV-1915)] - Parenthesis break enrichment expression parsing for non primary relations
* \[[DEV-1965](https://wmpartners.atlassian.net/browse/DEV-1965)] - Kestra: Output Delete on aggregation
* \[[DEV-2006](https://wmpartners.atlassian.net/browse/DEV-2006)] - Azure Copy APIs not handling files bigger than 256(?)MB
* \[[DEV-2029](https://wmpartners.atlassian.net/browse/DEV-2029)] - Failure on refresh
* \[[DEV-2032](https://wmpartners.atlassian.net/browse/DEV-2032)] - SDI: Cleanup Not Running
* \[[DEV-2033](https://wmpartners.atlassian.net/browse/DEV-2033)] - SDI: Effective Range Calculation Timeout

&#x20;Quick Fix

* \[[DEV-1657](https://wmpartners.atlassian.net/browse/DEV-1657)] - Add output\_source\_description to output\_sources: \[] for output mapping screen APIs
* \[[DEV-1913](https://wmpartners.atlassian.net/browse/DEV-1913)] - AGG functions are not included in output imports
* \[[DEV-1997](https://wmpartners.atlassian.net/browse/DEV-1997)] - Non-primary :M relation chain is grayed-out in enrichment attribute dropdown
* \[[DEV-2002](https://wmpartners.atlassian.net/browse/DEV-2002)] - Non-Primary Self Relations show up twice in relation drop down of rule expressions
* \[[DEV-2005](https://wmpartners.atlassian.net/browse/DEV-2005)] - On the UI, the 'Enrichment Name' parameter of Rules should be changed to 'Rule Name'
* \[[DEV-2009](https://wmpartners.atlassian.net/browse/DEV-2009)] - Optimize full source refresh
* \[[DEV-2010](https://wmpartners.atlassian.net/browse/DEV-2010)] - Sparky main optimization
* \[[DEV-2022](https://wmpartners.atlassian.net/browse/DEV-2022)] - Attributes that get repeated within relation expressions are duplicated in query gen
* \[[DEV-2034](https://wmpartners.atlassian.net/browse/DEV-2034)] - Nothing prevents users from making relations with duplicate names
