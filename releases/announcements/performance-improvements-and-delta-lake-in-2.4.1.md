# Performance Improvements and Delta Lake in 2.4.1

## Intellio DataOps 2.4.1 Release&#x20;

Similar to 2.4.0, this release primarily focuses on quality of life features – as well as a few important performance improvements and bugfixes. Here are a few of the major additions:&#x20;

* Performance and Stability Optimizations&#x20;
* Combined Source List Filter&#x20;
* Delta Lake Output&#x20;
* External API Routes&#x20;

## Data Processing Performance and Storage Optimizations&#x20;

_By: Vadim Orlov, Innovation Fellow and Intellio Labs Lead Architect_

When helping clients debug stability issues in 2.4.0, we noticed a number of storage patterns that Spark was using as default were causing significant performance, scalability, and stability issues.

We're providing a deep dive overview here for those interested, but the summary boils down to a restructuring of the raw files stored in the datalake which leverages metadata about the dynamically allocated infrastructure, refresh type, and a new target file size parameter to do some math and auto-optimize the storage layer more effectively to improve scalability on large datasets.

Some good pre-reading to this deep-dive is material on [Partitioning ](https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/)and [Bucketing ](https://sparkbyexamples.com/apache-hive/hive-bucketing-explained-with-examples/)in Hive. These concepts are foundational to all Datalake storage technologies - including Databricks' DeltaLake.

### Keyed Refresh Source Hub Table Optimization

#### **2.4.0 Default Spark Behavior**

Keyed Refresh Hub tables were bucketed on s\_key into fixed number of buckets:

```
(# of spark worker nodes) * 4
```

#### &#x20;Issues with Default Behavior:

Complex processing logic (e.g. window functions, aggregate lookups) would cause the spark engine to arbitrarily repartition the data into a default partition count (200) for optimal processing (e.g. using partition columns used by each window function). This often resulted in significant skewness of the data distribution across the individual raw parquet files stored in the datalake: &#x20;

* 10% of file count are very large (several GB each) and represent 90% of data&#x20;
* 90% of file count are very small (< 10MB) and represent 10% of data&#x20;
* Random distribution of files across buckets &#x20;

The existing behavior also failed to leverage the number of available executors or cluster node type for custom-configured clusters, which would optimally be aligned to the number of files generated by the partitioning and/or bucketing scheme.

This skeweness significantly extended data processing time and prevented cluster scaling due to uneven partitioning. Majority of executors assigned small partitions would finish quickly and idle, and few executors handling large partitions would take very long time to finish. This also impacted stability of the cluster as large partitions would overload node RAM and require frequent memory to disc spills. Both read and write performance is impacted by this. &#x20;

#### 2.4.1 Improved DataOps Optimized Solution &#x20;

1. Added a new system\_configuration parameter: target-parquet-file-size, which is defaulted to 256MB&#x20;
2. DataOps now captures and stores the size of each source hub table (visible on source list UI page)  &#x20;
3. Updated partitioning formula to include the available number of executors by detecting the number of available cluster CPUs at runtime
4. Calculated optimal target number of buckets based on above values of 1, 2, and 3 above.
   1. For example: A source with a Source hub\_table size of 10G and a default parquet size of 256M will be saved into 40 buckets, with 1 parquet file per bucket&#x20;
5. Added source parameter to disable bucketing (_**EXPERIMENTAL! USE WITH CAUTION**_)&#x20;
6. Added a new cluster size check as part of processing that generates a warning log when DataOps detects that the cluster worker count or node type is mis-sized for the calculated # of partitions.
   1. Ideal target: 1 executor per dataframe partition&#x20;

### Partitioned Hub Table Source Hub Table Optimization

None, Timestamp and Sequence Refresh Type with Keep Current attributes&#x20;

#### **2.4.0 Default Spark Behavior**

Before overwriting the hub table, the final spark Dataframe was coalesced into

```
 (# of spark worker nodes) * 4
```

partitions. The final Hive table was then partitioned on s\_input\_id&#x20;

#### Issues with Default Behavior:

As the size of hub table and number of batches (inputs) grow, the number of dataframe partitions increase, and the size of each partition becomes non-optimal (too small), causing either the [small file problem](https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908), or over-sized partitions: &#x20;

* Partition sizes are too large, leading to large files &#x20;
* Each partition contains mix of data from multiple batches (s\_input\_id)&#x20;

The existing behavior also failed to leverage the number of available executors or cluster node type for custom-configured clusters, which would optimally be aligned to the number of files generated by the partitioning and/or bucketing scheme.

Similar to the Keyed Refresh explanation above, uneven partitioning leads to skewness in processing and inefficient parquet file sizes. Some executors are busy writing multiple small files across multiple hive table partitions, others are overloaded by writing single large file and take very long time to finish. This also impacts cluster stability in the same manner as described for keyed sources. Both read and write performance is impacted by this. &#x20;

#### 2.4.1 Improved DataOps Optimized Solution &#x20;

1. Added a new system\_configuration parameter: target-parquet-file-size, which is defaulted to 256MB&#x20;
2. DataOps now captures and stores the size of each source hub table (visible on source list UI page)  &#x20;
3. Updated partitioning formula to include the available number of executors by detecting the number of available cluster CPUs at runtime
4. Added cluster size check that writes warning in log when cluster size is over- or undersized for the calculated # of partitions. Ideal target: 1 executor per dataframe partition&#x20;

Using new added parameters & calculations described in 1-3 above, calculate the target number of partitions to generate optimal file sizes.

For example, a source with a hub\_table size of 10G, partitioned into 10 batches (input\_ids) with default target parquet size 256M will be saved into 10 hive table partitions, with 4 parquet file in each partition&#x20;

### CDC & Enriched file partitioning and format optimization&#x20;

**2.4.0 Default Spark Behavior**

CDC DataLake files were saved in Avro serialization. Both CDC and Enriched files were split into the following chunks:

```
(# of spark worker nodes) * 4
```

#### Issues with Default Behavior:

The Avro file type is roughly 50% less performant vs. Parquet when performing full read-write set based operations, primarily due to Avro file sizes being larger.  The static number of partitions - (# of spark worker nodes) \* 4 - was optimized for 4 core nodes, and was incompatible and inefficient any custom cluster configuration.&#x20;

This resulted in a \~50% performance impact on read-write between CDC and Enrichment phases. Nodes with less than 4 cores generated too many files and nodes larger than 4 cores generated too few files.

#### 2.4.1 Improved DataOps Optimized Solution &#x20;

Updated formula to calculate the available number of executors using the number of available cluster CPUs at runtime

1. Changed output file format for CDC phase to parquet&#x20;
2. Added automatic detection of file type (Avro/Parquet) during Enrichment
   1. This prevents the need for any data migration or re-processing

## Improved Auto-Refresh of Source Status, Process, and Inputs pages

_By: Vadim Orlov, Innovation Fellow and Intellio Labs Lead Architect_

As part of the 6/21 Hotfix for 2.4.0, we were forced to temporarily disable the auto-refresh functionality due to it's impact on overall platform stability. Because manually refreshing the pages is an extremely painful user experience, we quickly designed not only a fix - but an improvement to this critical feature.

#### 2.4.0 Behavior:&#x20;

Many of the tables/pages in the UI were fully refreshed every five seconds on a timer

#### Issues with Default Behavior:

Process and Source status queries are complex, and when multiple users have these tabs open, a high volume of these queries can overload the API or Postgres Meta database, leading to an API crash and restart. &#x20;

Not only can this cause API crashes and disrupt the User experience, but the API also services Spark job communications, thus adding risk of failing of active spark jobs

#### 2.4.1 Solution &#x20;

Modified the source status recalculation from “every 5 sec per active UI user” to “recalculate when any source process changed state”

1. Persisted Source level status in the existing Sources postgres table and added it to Source UI page &#x20;
2. For Source level Inputs and Process tabs, changed data refresh mechanism from 5 second timer to “refresh only when source status changed”&#x20;
3. Added indexing on process\_history table to optimize refresh queries when viewed from Source level Process tab&#x20;
4. For top level process page, kept the 5 second refresh cadence and modified the code to only resend results when process data changed&#x20;

## Combined Source List Filter&#x20;

_By: Mary Scale - Experienced Consultant, UI/UX Engineering Developer_

![Previous Source Filters](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LhufZT729fit8K2vT1H-887967055%2Fuploads%2FsxVqlpgLTIxYxHMS9BGI%2Ffile.png?alt=media)

In previous versions, source filters were available as individual dropdown selections. This not only crowded the button bar, but also made it impossible to stack multiple values for one filter type.&#x20;

Now, source filters have been combined into two dropdowns: filter type, and filter sources.

Users will select a filter type in the first dropdown before selecting a value in the second. The filter sources dropdown also enables users to search for values; this is especially useful for filter types that have many values.&#x20;

Once a filter value has been selected, a filter tag appears in the brand-new Active Filters: bar. These tags display both the filter type and filter value. Users can have multiple filter tags of varying types or even the same filter type with different values!&#x20;

![](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LhufZT729fit8K2vT1H-887967055%2Fuploads%2F4CZANHxgYYviHkfctc0f%2Ffile.png?alt=media)

As shown above, there are three active filters. Two are group filters and the other is a refresh type. Only sources with a refresh type of key and group of either group value are displayed.

This flexibility allows users to create even more filter combinations than before.&#x20;

These filter tags can be removed by clicking their individual close icon. The tags also alternate between three Intellio DataOps colors in order to better distinguish them from one another.&#x20;

A final note on this new feature, just as with source names, users need not fear of losing their filter tags. Should a user navigate to a source then hit the back button, all their previously selected filter tags will still be present and displayed. Happy filtering!&#x20;

## Delta Lake Output

_By: Joe Swanson - Senior Consultant, Application & Infrastructure Engineering Lead _

Users can now create and write to Databricks Delta Lake tables using the Output features in the UI. Before, the only way to create a Hive output in Databricks was by using the Virtual Output feature - however, the virtual output only created a view in the Databricks metastore. Now, by using the Delta Lake Output feature, users can create a persisted Hive output table in Databricks that is optimized with Delta Lake capabilities.&#x20;

To get started working with Delta Lake Output, create a connection in your DataOps environment that points at an existing database in your Databricks hive metastore.&#x20;

![](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LhufZT729fit8K2vT1H-887967055%2Fuploads%2FoGZ1xok7nSEEX0EyH6cG%2Ffile.png?alt=media)

Once a connection is created, create the Output and Mappings in the same way that other Table outputs are created.&#x20;

{% hint style="warning" %}
The Table Schema parameter is deprecated for Delta Lake outputs. Please populate this field with a placeholder value for now.
{% endhint %}

Users can specify partition ordinals for columns in their mappings to control partitioning and partition column order. Columns that have partition ordinals on them will have a “PX” icon on their header, with X being the ordinal number configured in the column details.&#x20;

![](https://firebasestorage.googleapis.com/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-LhufZT729fit8K2vT1H-887967055%2Fuploads%2FQpnozs9u8T9Yy6nQwwXH%2Ffile.png?alt=media)

Schema restrictions on Delta Lake tables and base technology require DataOps to fully delete and overwrite the table when certain changes are made in the Output Mappings page. Please be careful!

These changes are:&#x20;

1. Partition Ordinal Changes
2. Column Name Changes
3. Data Type Changes

If the table does need to be overwritten due to these changes, don’t fear! There is logic to kick off output processes on all output channels that are connected to the output, so that the table will be completely refreshed without a loss of any data from its channels.&#x20;

Learn more about Databricks Delta Lake here: [https://docs.databricks.com/delta/index.html](https://docs.databricks.com/delta/index.html)&#x20;

## External API Routes&#x20;

_By: Joe Swanson - Senior Consultant, Application & Infrastructure Engineering Lead _

We’ve exposed 4 routes on our API application that can be accessed with a new Auth0 Machine to Machine application that is created by Terraform in the DataOps Auth0 tenant. The application will be called \<environment>-External-\<client>. Request a JWT token using the Client ID and Client Secret of the new external application and these routes will be accessible.&#x20;

#### Routes:

POST /external/source-pull/\<source-id>&#x20;

* Triggers a source to do a “pull now” and begin the Ingestion process&#x20;

POST /external/inputs-filtered&#x20;

* Returns list of inputs filtered by limit and effective status&#x20;
* Requires body: {"source\_id": \<source-id>, "limit": \<integer>, "effective\_filter": \<boolean>}&#x20;

GET /external/sources&#x20;

* Returns unfiltered list of sources in the environment&#x20;

GET /external/sources-filtered-by-name/\<filter>&#x20;

* Returns list of sources with filter condition applied to the source name field. The filter parameter should be a section of the source name, for example, if you were looking for all sources that had demo in the name, you would call /external/sources-filtered-by-name/demo&#x20;

## Specify Target Database for Virtual Outputs&#x20;

_By: Joe Swanson - Senior Consultant, Application & Infrastructure Engineering Lead _

Virtual Outputs can now be created in any database in the Databricks metastore. Use the “View Database” parameter in the Output settings page to define the database. If left blank, the database will be defaulted to the DataOps hub database.&#x20;
