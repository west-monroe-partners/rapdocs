# 2.5.0 New Features and Performance Optimization

## Intellio DataOps 2.5.0 Release

The 2.5.0 release is here. Several new features and bugfixes have been added to this release. All of the details of the release can be found in the official 2.5.0 release notes. Here are some of the major changes to come with 2.5.0:&#x20;

* [New Sources Tab – Raw Schema ](2.5.0-new-features-and-performance-optimization.md#new-sources-tab-raw-schema)
* [Output Mapping Saving Makeover  ](2.5.0-new-features-and-performance-optimization.md#output-mapping-saving-makeover)
* [Centralized Databricks Job & Cluster Configuration ](2.5.0-new-features-and-performance-optimization.md#centralized-databricks-job-and-cluster-configuration)
* [Resource Management 3.0 ](2.5.0-new-features-and-performance-optimization.md#resource-management-3.0)
* [Structured Datalake Performance Optimization ](2.5.0-new-features-and-performance-optimization.md#structured-datalake-performance-optimization)
* [Hardened Communications for Spark Jobs ](2.5.0-new-features-and-performance-optimization.md#hardened-communications-for-spark-jobs)
* [Improved processing time for Keyed CDC/Refresh reset operations ](2.5.0-new-features-and-performance-optimization.md#improved-processing-time-for-keyed-cdc-refresh-reset-operations)
* [Disabled concurrent runs for hub table check and optimized table partitioning ](2.5.0-new-features-and-performance-optimization.md#disabled-concurrent-runs-for-hub-table-check-and-optimized-table-partitioning)
* [Change result file type from Avro to parquet for custom ingestion, custom & sparky parse ](2.5.0-new-features-and-performance-optimization.md#change-result-file-type-from-avro-to-parquet-for-custom-ingestion-custom-and-sparky-parse)
* [Performance optimization: use cached results from the prior process ](2.5.0-new-features-and-performance-optimization.md#performance-optimization-use-cached-results-from-the-prior-process)
* [Mini Sparky and Data Viewer clusters are now separate and created via Terraform ](2.5.0-new-features-and-performance-optimization.md#mini-sparky-and-data-viewer-clusters-are-now-separate-and-created-via-terraform)
* [AWS Private Environment UI is now served from a container instead of static S3 website   ](2.5.0-new-features-and-performance-optimization.md#aws-private-environment-ui-is-now-served-from-a-container-instead-of-static-s3-website)
* [Ability to use existing VPC for DataOps AWS infrastructure deploy ](2.5.0-new-features-and-performance-optimization.md#aws-private-environment-ui-is-now-served-from-a-container-instead-of-static-s3-website)
* [Usage Agent is deployed to collect environment usage metrics ](2.5.0-new-features-and-performance-optimization.md#usage-agent-is-deployed-to-collect-environment-usage-metrics)
* [Databricks 9.1 Support Available](2.5.0-new-features-and-performance-optimization.md#databricks-9.1-support-available)

### New Sources Tab – Raw Schema&#x20;

#### Raw schema tab added to Sources&#x20;

_By: Mary Scale – Experienced Consultant_&#x20;

![Raw Schema Tab ](../../.gitbook/assets/raw\_schema1.png)

As of 2.5.0, Sources now have a new tab, Raw Schema. This tab displays the raw database schema for the individual source. &#x20;

Users can view schema details on every mapped column as well as edit column descriptions. To edit a column description, simply click on the description column for the desired row. &#x20;

![Raw Metadata popup](../../.gitbook/assets/raw\_schema2.png)

In addition, users can now view raw metadata for individual columns. If metadata is available, users view it by clicking on the icon within the Raw Metadata column. A popup will appear containing all raw metadata associated with the column.

### Output Mapping Saving Makeover&#x20;

#### New function design for the output mapping tab&#x20;

_By: Mary Scale – Experienced Consultant_&#x20;

The Output mapping screen has many moving parts. To maintain data integrity, the Output Mapping tab now saves after every major function. This means the confirm buttons and the global save button have been removed. The replacement is single save buttons or automatic saves after any all edits to data.&#x20;

For instance, when a user auto-maps columns, they will automatically be saved. The same is true when users click, Add All Columns. All popup windows in output mappings now have save buttons which allow users to immediately save after they have made changes.&#x20;

These updates help prevent users from overwriting one another’s changes. Before, if two users were working on the same output mapping, the first users saved changes would be corrupted with the second user’s changes since the page allowed multiple changes at once. Now, users can safely work in parallel on the same mappings.&#x20;

### Centralized Databricks Job & Cluster Configuration  &#x20;

_By: Vadim Orlov_&#x20;

There are now two new top level configuration objects: Cluster and Process Configuration. These are found under the System Configuration in the main menu. &#x20;

Cluster Configuration contains all settings and parameters required to create Databricks [Job,](https://docs.databricks.com/jobs.html#create-a-job) plus, several IDO-specific parameters to configure error recovery and parallelism. Each Cluster configuration is immediately saved as a Databricks job. Job\_ID hyperlink allow users to navigate from IDO to Databricks UI to view job details, history and execution logs. &#x20;

Process Configuration is comprised of default Cluster Configuration and Cluster Configuration overrides for specific process types. For example, a user can set a different cluster configuration for Enrichment and Refresh process types, allocating more compute capacity to the more resource-intensive process types. Process Configuration is now a required attribute of a Source, configured on the Source Settings UI tab.&#x20;

Custom process types are now configured as Custom Cluster Configurations. They are attached directly to a Source (Custom Ingestion and Parse) or Output (Post Output). Custom Cluster Configuration connects the notebook that will be executed. Notebooks are now easily accessible via a hyperlink from Cluster Configuration settings page.&#x20;

### Resource Management 3.0  &#x20;

_By: Vadim Orlov_&#x20;

Pre-2.5, new Databricks Jobs were dynamically created every time source processing was required. This led to several issues: &#x20;

* Databricks quickly exceeded its limit of 1500 jobs per environment and had to execute frequent clean-ups to remove history; this impacted the ability to review execution logs, troubleshoot and optimize jobs.&#x20;
* Any job created immediately before the start of processing, any mis-configuration issues (e.g. incorrect parameter values in source custom cluster configuration) would result in critical process failures that were difficult to diagnose.&#x20;
* Once the job was created, it was rigidly tied to the source – different job and cluster configurations for specific process types were not previously supported. This was especially impactful for custom process types (ingestion, parse, post-output) because it forced all standard IDO processes to use cluster configured for the custom process type.&#x20;

Summary of improvements made in 2.5: &#x20;

* IDO creates Databricks Job immediately when user clicks, “Save” on the Cluster Configuration Settings screen. Job settings are instantly validated, and any misconfiguration errors are communicated to the user&#x20;
* Instead of creating a new job every time, IDO Resource Manager creates new [Job Run](https://docs.databricks.com/jobs.html#view-job-runs) linked to the Job defined by Cluster Configuration. Job runs are replicated from Databricks API to IDO meta.job\_run and history.job\_run postgres tables every fifteen seconds. Databricks retains a complete history of job runs for sixty days, including parameters, events, logs, Spark UI and load dashboards. This data is accessible via hyperlink on the IDO Source Process tab&#x20;
* The max\_heartbeat\_wait\_time Source parameter is now deprecated. Instead of waiting for static timeout, IDO now uses near-real time data from job\_run table to detect and log job run launch issues faster and trigger retry&#x20;
* Consolidated launching, stopping and re-starting job runs into Core Resource Manager&#x20;
* Added stop\_reason to job\_run table to improve transparency and help troubleshooting &#x20;
* Custom Ingestion sources now enqueue retries when they fail in the custom ingestion step&#x20;

### Structured Datalake Performance Optimization  &#x20;

_By: Vadim Orlov_&#x20;

Pre-2.5 release, IDO optimized size of each individual parquet file (partition) for hub tables using the value of target-parquet-file-size global parameter (default 256MB) stored in meta.system\_configuration table. &#x20;

In this release, this is extended to all layers of the Datalake from parsed through hub table files. This will improve processing performance of large batches of data (inputs) across all process types from CDC through Attribute Recalculation by optimizing spark read and write performance, parquet compression and minimizing data skew in spark dataframe partitions.  &#x20;

This also enables cost-effective databricks cluster auto-scaling as number of file partitions is determined by size of the batch and source data and is no longer tied to the number of compute cores configured for the cluster.&#x20;

### Hardened Communications for Spark Jobs    &#x20;

_By: Vadim Orlov_&#x20;

Pre-2.5, each spark job communicated with IDO database using an API layer. This often led to instability under high load and caused jobs to be aborted due to transient API load balancer errors, resulting in increased costs for reprocessing. &#x20;

New Databricks [E2 architecture](https://docs.databricks.com/security/security-overview-e2.html) simplifies spark job communications, pointing directly to the Postgres database and bypassing the API layer. This change enabled several benefits:&#x20;

* Improved stability, eliminating API layer as the point of failure&#x20;
* Reduced reprocessing costs&#x20;
* Reduced API load, costs, and improved UI performance and user experience under high load&#x20;
* Enabled dynamic scale of Aurora RDS instance with load, reducing AWS costs&#x20;

### Improved processing time for Keyed CDC/Refresh reset operations    &#x20;

_By: Vadim Orlov_&#x20;

Pre-2.5 resetting CDC or deleting input in source with key refresh took a long time; this required rollback process and sequentially executed CDC reset operations for each individual input which could take several hours or even days on a source with many inputs. &#x20;

Now, there is a new manual\_reset\_all\_processing\_from\_cdc process type to optimize performance of these operations. As a result, the rollback process type is now deprecated. &#x20;

Both “input delete” and “reset all CDC” operations use the new process type and are now performed as a single spark job. For input delete, the job reads all CDC files, recalculates record\_status\_code, performs enrichment and overwrites hub and hub\_history tables. Reset All CDC starts with parsed files and recalculates key and s\_update\_timestamp CDC columns before following the same process.&#x20;

To have the best processing performance for large sources, users are recommended to have auto-scaling cluster with 4-8 core node size and large max number of nodes, minimum ten, configured for this operation.&#x20;

### Disabled concurrent runs for hub table check and optimized table partitioning      &#x20;

_By: Vadim Orlov_&#x20;

Two known issues have been fixed: &#x20;

* Under high load, IDO could execute concurrent hub table check operations on the same hub table. This could lead to race condition and risk of table corruption or data duplication&#x20;
* Hub table check operation ignored target-parquet-file-size global parameter, overwriting the table with number of partitions equal to total number of spark cluster cores. This resulted in sub-optimal table partitioning and data skew&#x20;

### Change result file type from Avro to parquet for custom ingestion, custom & sparky parse      &#x20;

_By: Vadim Orlov_&#x20;

This improves performance by 50-80% over the write-read cycle. The result is smaller and better compressed files. The system will now automatically recognize the file type and handle legacy avro-formatted files.&#x20;

### Performance optimization: use cached results from the prior process        &#x20;

_By: Vadim Orlov_&#x20;

Instead of reading the source file generated by prior processing phase, like CDC, IDO now uses cached results of the previous step. &#x20;

Once the system confirms that the previous step was executed on the same spark cluster, file read will be skipped. Instead, the cached dataframe results from the previous process step will be used. &#x20;

This is enabled for sparky parse, CDC enrichment and refresh phases which provides significant performance increase.&#x20;

### Mini Sparky and Data Viewer clusters are now separate and created via Terraform &#x20;

_By: Joe Swanson – Senior Consultant_&#x20;

In 2.5.0, the mini-sparky cluster that was used to make connections from the UI to Databricks has now been split into two clusters, ido-mini-sparky and ido-data-viewer. Now, all Data Viewer requests will go to the ido-data-viewer cluster and all other requests will go through the ido-mini-sparky cluster. &#x20;

This provides users with more stability while working in the platform, as large requests to the Data Viewer page will not interfere with rules and relation creation. If there are any rap-mini-sparky clusters in the Databricks environment after upgrading to 2.5.0, they can be deleted.&#x20;

### AWS Private Environment UI is now served from a container instead of static S3 website&#x20;

_By: Joe Swanson – Senior Consultant_&#x20;

Serving the UI files from an S3 static website bucket led to challenges around privatization and security, namely AWS does not support adding HTTPS to a static S3 website. &#x20;

The S3 bucket has been removed, it will need to be emptied and deleted during the Terraform upgrade. In addition, a container named ui has been created that will be added to the ECS cluster. The container will resolve to the same UI DNS as before and traffic will be routed through a new route in the Load Balancer. The ECS container will be attached to the ALB that currently serves the API via a new target group - similar to how the API container is currently set up. There may be a need to edit the security group that is attached to the ALB to allow VPN or RDP traffic to the ALB. Please reach out to West Monroe infrastructure team for guidance on updating networking for the new UI container.

After the upgrade, some networking adjustments may need to be made to allow VPN/Private network traffic to the security group that the container is using.&#x20;

### Ability to use existing VPC for DataOps AWS infrastructure deploy&#x20;

_By: Joe Swanson – Senior Consultant_&#x20;

Infrastructure teams can now utilize existing VPC, and other existing resources within the VPC, to fully customize the networking elements of the DataOps infrastructure. &#x20;

Please refer to the variables.tf file in the aws/main-deployment section of the infrastructure repository in GitHub to see the variables that can be customized – they will all generally start with “existing”, such as existingVPCId.&#x20;

### Usage Agent is deployed to collect environment usage metrics&#x20;

_By: Joe Swanson – Senior Consultant_&#x20;

There is a new service added to the ECS cluster in AWS and a new container instance added to the Resource Group in Azure, called usage-agent. This service will be used to collect information from the Postgres metastore database and send to West Monroe for usage tracking and pricing. &#x20;

There are two variables that need added to Terraform; please, refer to the 2.5.0 upgrade guide in GitBook for more information.&#x20;

### Databricks 9.1 Support Available &#x20;

_By: Jacob Crell  – Senior Consultant_&#x20;

IDO now supports Databricks Runtime 9.1 for running all process types! Our internal testing has shown Databricks 9.1 to be substantially faster and cheaper than the Databricks 7.3 runtime utilized in IDO versions 2.4.3 and earlier. Instructions for setting up IDO sources to run on Databricks version 9.1 can be found [here.](https://docs.dataops.intellio.wmp.com/dataops/v/master/user-manual/system-configuration/cluster-and-process-configuration-overview/cluster-configuration/cluster-configuration-example-databricks-9.1)&#x20;

Our team is aware of a few limitations in Databricks Runtime 9.1 related to SQL Server Outputs. If the 9.1 Runtime is causing failures, see [here](https://docs.dataops.intellio.wmp.com/dataops/v/master/user-manual/system-configuration/cluster-and-process-configuration-overview/process-configuration/process-override-example-databricks-9.1) for instructions to switch individual process types to use Runtime 7.3 again.&#x20;
